{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xSmku2iJ5Ja"
      },
      "source": [
        "# Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq0whze7J4D_"
      },
      "outputs": [],
      "source": [
        "!pip install -U pynwb --quiet\n",
        "!pip install wget --quiet\n",
        "!pip install pytorch-lightning --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQD6kgT-KG9i"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6LGyVGWKK5z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io.wavfile as wavfile\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aEap4KvKbXJ"
      },
      "source": [
        "# Download and unpack data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bImek2lKcSQ"
      },
      "outputs": [],
      "source": [
        "wget.download(\"https://osf.io/download/g6q5m/\", \"SingleWordProductionDutch-iBIDS.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3rY78pwKkHW"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('SingleWordProductionDutch-iBIDS.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall()\n",
        "os.remove('SingleWordProductionDutch-iBIDS.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZLE03vNKl-M"
      },
      "source": [
        "# Processing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mocCdw1JKmRQ"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/eva-vision/2BRAINS/main/MelFilterBank.py --quiet\n",
        "!wget https://raw.githubusercontent.com/eva-vision/2BRAINS/main/extract_features.py --quiet\n",
        "!wget https://raw.githubusercontent.com/eva-vision/2BRAINS/main/reconstructWave.py --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVAAlOipKq7E"
      },
      "outputs": [],
      "source": [
        "!python MelFilterBank.py\n",
        "!python reconstructWave.py --quiet\n",
        "!python extract_features.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnF4UZzRNssv"
      },
      "outputs": [],
      "source": [
        "import reconstructWave as rW\n",
        "import MelFilterBank as mel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWp8i7bQKzmu"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Network"
      ],
      "metadata": {
        "id": "sCvEntUP5R8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUs1jvP0K0Wn"
      },
      "outputs": [],
      "source": [
        "# Define the neural network architecture\n",
        "class TimeSeriesNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TimeSeriesNet, self).__init__()\n",
        "\n",
        "        # Input layer\n",
        "        self.input = nn.Linear(1143, 512)\n",
        "\n",
        "        # Hidden layers\n",
        "        self.hidden1 = nn.Linear(512, 256)\n",
        "        self.hidden2 = nn.Linear(256, 256)\n",
        "        self.hidden3 = nn.Linear(256, 128)\n",
        "        self.hidden4 = nn.Linear(128, 64)\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Linear(64, 23)\n",
        "\n",
        "        # Batch normalization layers\n",
        "        self.bn_input = nn.BatchNorm1d(512)\n",
        "        self.bn_hidden1 = nn.BatchNorm1d(256)\n",
        "        self.bn_hidden2 = nn.BatchNorm1d(256)\n",
        "        self.bn_hidden3 = nn.BatchNorm1d(128)\n",
        "        self.bn_hidden4 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input layer with activation and normalization\n",
        "        x = F.relu(self.bn_input(self.input(x)))\n",
        "\n",
        "        # Hidden layers with activation and normalization\n",
        "        x = F.relu(self.bn_hidden1(self.hidden1(x)))\n",
        "        x = F.relu(self.bn_hidden2(self.hidden2(x)))\n",
        "        x = F.relu(self.bn_hidden3(self.hidden3(x)))\n",
        "        x = F.relu(self.bn_hidden4(self.hidden4(x)))\n",
        "\n",
        "        # Output layer\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5qknewGK558"
      },
      "outputs": [],
      "source": [
        "feat_path = r'./features'\n",
        "os.makedirs(os.path.join(result_path), exist_ok=True)\n",
        "result_path = r'./results'\n",
        "pt = 'sub-01'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxEw-75qK_Cy"
      },
      "outputs": [],
      "source": [
        "_data = np.load(os.path.join(feat_path,f'{pt}_feat.npy'))\n",
        "_spectrogram = np.load(os.path.join(feat_path,f'{pt}_spec.npy'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline training"
      ],
      "metadata": {
        "id": "rZNAga0_6Jde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(_data, _spectrogram, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "j8IP_-s15i2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxqnv3afLBaH"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # Load data\n",
        "        self.X = X_train\n",
        "        self.y = y_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)\n",
        "\n",
        "dataset = TimeSeriesDataset()\n",
        "\n",
        "# Create data loader\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgf3AH4OLD34"
      },
      "outputs": [],
      "source": [
        "# Define a loss function and optimizer\n",
        "time_series_net_baseline = TimeSeriesNet()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss, common for regression tasks\n",
        "optimizer = optim.Adam(time_series_net_baseline.parameters(), lr=0.001)  # Using Adam optimizer\n",
        "\n",
        "# Function to train the network\n",
        "def train_network(net, dataloader, epochs=5):\n",
        "    net.train()  # Set the network into training mode\n",
        "\n",
        "    for epoch in range(epochs):  # Loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # Get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:    # Print every 100 mini-batches\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "# Example usage\n",
        "train_network(time_series_net_baseline, dataloader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxDrYBiHLNQN"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5FdM5LLLKnn"
      },
      "outputs": [],
      "source": [
        "def infer(net, input_data):\n",
        "    net.eval()  # Set the network to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
        "        inputs = torch.tensor(input_data, dtype=torch.float32)\n",
        "\n",
        "        # Check if the network is on a GPU and move the inputs to the same device\n",
        "        if next(net.parameters()).is_cuda:\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        return outputs\n",
        "\n",
        "# Example usage\n",
        "# input_data = [array of your new input data]  # Replace with your input data\n",
        "predictions_baseline = infer(time_series_net_baseline, _data)\n",
        "print(predictions_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "wDqsbnUoAw2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics for the baseline model\n",
        "baseline_mae = mean_absolute_error(_spectrogram, predictions_baseline)\n",
        "baseline_mse = mean_squared_error(_spectrogram, predictions_baseline)\n",
        "baseline_r2 = r2_score(_spectrogram, predictions_baseline)\n",
        "# Create a DataFrame to store the results\n",
        "baseline_results = pd.DataFrame({'MAE': [baseline_mae], 'MSE': [baseline_mse], 'R2 Score': [baseline_r2]})\n",
        "# Save the results to a CSV file\n",
        "baseline_results.to_csv(os.path.join(result_path, 'baseline_results.csv'), index=False)\n",
        "print(baseline_results)"
      ],
      "metadata": {
        "id": "17qT8mgXAw2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyIwIn0kLXU7"
      },
      "source": [
        "### Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HCo3oByLT1e"
      },
      "outputs": [],
      "source": [
        "# Viz example spectrogram\n",
        "#Which timeframe to plot\n",
        "start_s = 5.5\n",
        "stop_s = 19.5\n",
        "\n",
        "frameshift = 0.01\n",
        "#Load spectrograms\n",
        "rec_spec = predictions_baseline\n",
        "spectrogram = _spectrogram\n",
        "#Load prompted words\n",
        "#eeg_sr= 1024\n",
        "#words = np.load(os.path.join(feat_path,f'{participant}_procWords.npy'))[int(start_s*eeg_sr):int(stop_s*eeg_sr)]\n",
        "#words = [words[w] for w in np.arange(1,len(words)) if words[w]!=words[w-1] and words[w]!='']\n",
        "\n",
        "\n",
        "cm='viridis'\n",
        "fig, ax = plt.subplots(2, sharex=True)\n",
        "#Plot spectrograms\n",
        "pSta=int(start_s*(1/frameshift));pSto=int(stop_s*(1/frameshift))\n",
        "ax[0].imshow(np.flipud(spectrogram[pSta:pSto, :].T), cmap=cm, interpolation=None,aspect='auto')\n",
        "ax[0].set_ylabel('Log Mel-Spec Bin')\n",
        "ax[1].imshow(np.flipud(rec_spec[pSta:pSto, :].T), cmap=cm, interpolation=None,aspect='auto')\n",
        "plt.setp(ax[1], xticks=np.arange(0,pSto-pSta,int(1/frameshift)), xticklabels=[str(x/int(1/frameshift)) for x in np.arange(0,pSto-pSta,int(1/frameshift))])\n",
        "#plt.setp(ax[1], xticks=np.arange(int(1/frameshift),spectrogram[pSta:pSto, :].shape[0],3*int(1/frameshift)), xticklabels=words)\n",
        "ax[1].set_ylabel('Log Mel-Spec Bin')\n",
        "\n",
        "plt.savefig(os.path.join(result_path,'spec_example_baseline.png'),dpi=600)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJgZ8f3PLd3J"
      },
      "source": [
        "### Audio output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYqMPOQWQQvI"
      },
      "outputs": [],
      "source": [
        "def createAudio(spectrogram, audiosr=16000, winLength=0.05, frameshift=0.01):\n",
        "  \"\"\"\n",
        "  Create a reconstructed audio wavefrom\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  spectrogram: array\n",
        "  Spectrogram of the audio\n",
        "  sr: int\n",
        "  Sampling rate of the audio\n",
        "  windowLength: float\n",
        "  Length of window (in seconds) in which spectrogram was calculated\n",
        "  frameshift: float\n",
        "  Shift (in seconds) after which next window was extracted\n",
        "  Returns\n",
        "  ----------\n",
        "  scaled: array\n",
        "  Scaled audio waveform\n",
        "  \"\"\"\n",
        "  mfb = mel.MelFilterBank(int((audiosr*winLength)/2+1), spectrogram.shape[1], audiosr)\n",
        "  nfolds = 1\n",
        "  hop = int(spectrogram.shape[0]/nfolds)\n",
        "  rec_audio = np.array([])\n",
        "  for_reconstruction = mfb.fromLogMels(spectrogram)\n",
        "  for w in range(0,spectrogram.shape[0],hop):\n",
        "    spec = for_reconstruction[w:min(w+hop,for_reconstruction.shape[0]),:]\n",
        "  rec = rW.reconstructWavFromSpectrogram(spec,spec.shape[0]*spec.shape[1],fftsize=int(audiosr*winLength),overlap=int(winLength/frameshift))\n",
        "  rec_audio = np.append(rec_audio,rec)\n",
        "  scaled = np.int16(rec_audio/np.max(np.abs(rec_audio)) * 32767)\n",
        "  return scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGZMecDgLhLF"
      },
      "outputs": [],
      "source": [
        "audiosr = 16000\n",
        "winLength = 0.05\n",
        "frameshift = 0.01\n",
        "np.save(os.path.join(result_path,f'{pt}_predicted_spec_baseline.npy'), predictions_baseline)\n",
        "#Synthesize waveform from spectrogram using Griffin-Lim\n",
        "reconstructedWav = createAudio(predictions_baseline,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "wavfile.write(os.path.join(result_path,f'{pt}_predicted_baseline.wav'),int(audiosr),reconstructedWav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyzLZONFQa3n"
      },
      "outputs": [],
      "source": [
        "#For comparison synthesize the original spectrogram with Griffin-Lim\n",
        "origWav = createAudio(_spectrogram,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "wavfile.write(os.path.join(result_path,f'{pt}_orig_synthesized_baseline.wav'),int(audiosr),origWav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cgZWn_ZLnF0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "print(\"Original synthesized\\n\")\n",
        "display(Audio(os.path.join(result_path,f'{pt}_orig_synthesized_baseline.wav'), autoplay=False))\n",
        "print(\"\\nPredicted\\n\")\n",
        "display(Audio(os.path.join(result_path,f'{pt}_predicted_baseline.wav'), autoplay=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final training"
      ],
      "metadata": {
        "id": "uvwv01Sj7NBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = _data\n",
        "y = _spectrogram"
      ],
      "metadata": {
        "id": "385hG0iH8TfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjUd-G-LSFUd"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "batch_size = 32\n",
        "epoch_number = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "time_series_net_final = TimeSeriesNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(time_series_net_final.parameters(), lr=learning_rate)\n",
        "\n",
        "# Loop over each fold\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "    print(f\"Training on fold {fold+1}/5...\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Create Tensor datasets\n",
        "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
        "\n",
        "    # Create Data Loaders\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    #Train loop\n",
        "    time_series_net_final.train()\n",
        "    for epoch in range(epoch_number):  # Loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # Get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = time_series_net_final(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:    # Print every 100 mini-batches\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "                running_loss = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nio_1-d8iYV"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoJnEPc98iYV"
      },
      "outputs": [],
      "source": [
        "def infer(net, input_data):\n",
        "    net.eval()  # Set the network to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
        "        inputs = torch.tensor(input_data, dtype=torch.float32)\n",
        "\n",
        "        # Check if the network is on a GPU and move the inputs to the same device\n",
        "        if next(net.parameters()).is_cuda:\n",
        "            inputs = inputs.to('cuda')\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        return outputs\n",
        "\n",
        "# Example usage\n",
        "# input_data = [array of your new input data]  # Replace with your input data\n",
        "predictions_final = infer(time_series_net_final, _data)\n",
        "print(predictions_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "p-YM_0NGACW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics for the final model\n",
        "final_mae = mean_absolute_error(_spectrogram, predictions_final)\n",
        "final_mse = mean_squared_error(_spectrogram, predictions_final)\n",
        "final_r2 = r2_score(_spectrogram, predictions_final)\n",
        "# Create a DataFrame to store the results\n",
        "final_results = pd.DataFrame({'MAE': [final_mae], 'MSE': [final_mse], 'R2 Score': [final_r2]})\n",
        "# Save the results to a CSV file\n",
        "final_results.to_csv(os.path.join(result_path, 'final_results.csv'), index=False)\n",
        "print(final_results)"
      ],
      "metadata": {
        "id": "cOQZNSRAABnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIsMC5j8iYW"
      },
      "source": [
        "### Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnYqV5uh8iYX"
      },
      "outputs": [],
      "source": [
        "# Viz example spectrogram\n",
        "#Which timeframe to plot\n",
        "start_s = 5.5\n",
        "stop_s = 19.5\n",
        "\n",
        "frameshift = 0.01\n",
        "#Load spectrograms\n",
        "rec_spec = predictions_final\n",
        "spectrogram = _spectrogram\n",
        "#Load prompted words\n",
        "#eeg_sr= 1024\n",
        "#words = np.load(os.path.join(feat_path,f'{participant}_procWords.npy'))[int(start_s*eeg_sr):int(stop_s*eeg_sr)]\n",
        "#words = [words[w] for w in np.arange(1,len(words)) if words[w]!=words[w-1] and words[w]!='']\n",
        "\n",
        "os.makedirs(os.path.join(result_path), exist_ok=True)\n",
        "\n",
        "cm='viridis'\n",
        "fig, ax = plt.subplots(2, sharex=True)\n",
        "#Plot spectrograms\n",
        "pSta=int(start_s*(1/frameshift));pSto=int(stop_s*(1/frameshift))\n",
        "ax[0].imshow(np.flipud(spectrogram[pSta:pSto, :].T), cmap=cm, interpolation=None,aspect='auto')\n",
        "ax[0].set_ylabel('Log Mel-Spec Bin')\n",
        "ax[1].imshow(np.flipud(rec_spec[pSta:pSto, :].T), cmap=cm, interpolation=None,aspect='auto')\n",
        "plt.setp(ax[1], xticks=np.arange(0,pSto-pSta,int(1/frameshift)), xticklabels=[str(x/int(1/frameshift)) for x in np.arange(0,pSto-pSta,int(1/frameshift))])\n",
        "#plt.setp(ax[1], xticks=np.arange(int(1/frameshift),spectrogram[pSta:pSto, :].shape[0],3*int(1/frameshift)), xticklabels=words)\n",
        "ax[1].set_ylabel('Log Mel-Spec Bin')\n",
        "\n",
        "plt.savefig(os.path.join(result_path,'spec_example_baseline.png'),dpi=600)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkK4PMC-8iYX"
      },
      "source": [
        "### Audio output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8u63pYB8iYY"
      },
      "outputs": [],
      "source": [
        "def createAudio(spectrogram, audiosr=16000, winLength=0.05, frameshift=0.01):\n",
        "  \"\"\"\n",
        "  Create a reconstructed audio wavefrom\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  spectrogram: array\n",
        "  Spectrogram of the audio\n",
        "  sr: int\n",
        "  Sampling rate of the audio\n",
        "  windowLength: float\n",
        "  Length of window (in seconds) in which spectrogram was calculated\n",
        "  frameshift: float\n",
        "  Shift (in seconds) after which next window was extracted\n",
        "  Returns\n",
        "  ----------\n",
        "  scaled: array\n",
        "  Scaled audio waveform\n",
        "  \"\"\"\n",
        "  mfb = mel.MelFilterBank(int((audiosr*winLength)/2+1), spectrogram.shape[1], audiosr)\n",
        "  nfolds = 1\n",
        "  hop = int(spectrogram.shape[0]/nfolds)\n",
        "  rec_audio = np.array([])\n",
        "  for_reconstruction = mfb.fromLogMels(spectrogram)\n",
        "  for w in range(0,spectrogram.shape[0],hop):\n",
        "    spec = for_reconstruction[w:min(w+hop,for_reconstruction.shape[0]),:]\n",
        "  rec = rW.reconstructWavFromSpectrogram(spec,spec.shape[0]*spec.shape[1],fftsize=int(audiosr*winLength),overlap=int(winLength/frameshift))\n",
        "  rec_audio = np.append(rec_audio,rec)\n",
        "  scaled = np.int16(rec_audio/np.max(np.abs(rec_audio)) * 32767)\n",
        "  return scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C23_s7UA8iYZ"
      },
      "outputs": [],
      "source": [
        "audiosr = 16000\n",
        "winLength = 0.05\n",
        "frameshift = 0.01\n",
        "np.save(os.path.join(result_path,f'{pt}_predicted_spec_final.npy'), predictions_final)\n",
        "#Synthesize waveform from spectrogram using Griffin-Lim\n",
        "reconstructedWav = createAudio(predictions_final,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "wavfile.write(os.path.join(result_path,f'{pt}_predicted_final.wav'),int(audiosr),reconstructedWav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "786XZWet8iYZ"
      },
      "outputs": [],
      "source": [
        "#For comparison synthesize the original spectrogram with Griffin-Lim\n",
        "origWav = createAudio(_spectrogram,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "wavfile.write(os.path.join(result_path,f'{pt}_orig_synthesized_final.wav'),int(audiosr),origWav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvZeqc6_8iYa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "print(\"Original synthesized\\n\")\n",
        "display(Audio(os.path.join(result_path,f'{pt}_orig_synthesized_final.wav'), autoplay=False))\n",
        "print(\"\\nPredicted\\n\")\n",
        "display(Audio(os.path.join(result_path,f'{pt}_predicted_final.wav'), autoplay=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble"
      ],
      "metadata": {
        "id": "LrFrPkIH88lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X_test to the appropriate data type and shape for predictions\n",
        "# Ensemble the predictions\n",
        "ensemble = 0.5 * predictions_baseline + 0.5 * predictions_final\n",
        "\n",
        "# Calculate metrics for the ensemble model\n",
        "ensemble_mae = mean_absolute_error(_spectrogram, ensemble)\n",
        "ensemble_mse = mean_squared_error(_spectrogram, ensemble)\n",
        "ensemble_r2 = r2_score(_spectrogram, ensemble)\n",
        "# Create a DataFrame to store the results\n",
        "ensemble_results = pd.DataFrame({'MAE': [ensemble_mae], 'MSE': [ensemble_mse], 'R2 Score': [ensemble_r2]})\n",
        "# Save the results to a CSV file\n",
        "ensemble_results.to_csv(os.path.join(result_path, 'ensemble_results.csv'), index=False)\n",
        "print(ensemble_results)"
      ],
      "metadata": {
        "id": "ho5Yd0Tr37jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVYjFmA-QHh"
      },
      "source": [
        "### Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9YoDknR-QHh"
      },
      "outputs": [],
      "source": [
        "# Viz example spectrogram\n",
        "#Which timeframe to plot\n",
        "start_s = 5.5\n",
        "stop_s = 19.5\n",
        "\n",
        "frameshift = 0.01\n",
        "#Load spectrograms\n",
        "rec_spec = ensemble\n",
        "spectrogram = _spectrogram\n",
        "#Load prompted words\n",
        "#eeg_sr= 1024\n",
        "#words = np.load(os.path.join(feat_path,f'{participant}_procWords.npy'))[int(start_s*eeg_sr):int(stop_s*eeg_sr)]\n",
        "#words = [words[w] for w in np.arange(1,len(words)) if words[w]!=words[w-1] and words[w]!='']\n",
        "\n",
        "os.makedirs(os.path.join(result_path), exist_ok=True)\n",
        "\n",
        "cm='viridis'\n",
        "fig, ax = plt.subplots(2, sharex=True)\n",
        "#Plot spectrograms\n",
        "pSta=int(start_s*(1/frameshift));pSto=int(stop_s*(1/frameshift))\n",
        "ax[0].imshow(np.flipud(spectrogram[pSta:pSto, :].T), cmap=cm, interpolation=None,aspect='auto')\n",
        "ax[0].set_ylabel('Log Mel-Spec Bin')\n",
        "ax[1].imshow(np.flipud(rec_spec[pSta:pSto, :].T), cmap=cm, interpolation=None,aspect='auto')\n",
        "plt.setp(ax[1], xticks=np.arange(0,pSto-pSta,int(1/frameshift)), xticklabels=[str(x/int(1/frameshift)) for x in np.arange(0,pSto-pSta,int(1/frameshift))])\n",
        "#plt.setp(ax[1], xticks=np.arange(int(1/frameshift),spectrogram[pSta:pSto, :].shape[0],3*int(1/frameshift)), xticklabels=words)\n",
        "ax[1].set_ylabel('Log Mel-Spec Bin')\n",
        "\n",
        "plt.savefig(os.path.join(result_path,'spec_example_baseline.png'),dpi=600)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbiX2pHk-QHj"
      },
      "source": [
        "### Audio output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-vMO5Sc-QHj"
      },
      "outputs": [],
      "source": [
        "def createAudio(spectrogram, audiosr=16000, winLength=0.05, frameshift=0.01):\n",
        "  \"\"\"\n",
        "  Create a reconstructed audio wavefrom\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  spectrogram: array\n",
        "  Spectrogram of the audio\n",
        "  sr: int\n",
        "  Sampling rate of the audio\n",
        "  windowLength: float\n",
        "  Length of window (in seconds) in which spectrogram was calculated\n",
        "  frameshift: float\n",
        "  Shift (in seconds) after which next window was extracted\n",
        "  Returns\n",
        "  ----------\n",
        "  scaled: array\n",
        "  Scaled audio waveform\n",
        "  \"\"\"\n",
        "  mfb = mel.MelFilterBank(int((audiosr*winLength)/2+1), spectrogram.shape[1], audiosr)\n",
        "  nfolds = 1\n",
        "  hop = int(spectrogram.shape[0]/nfolds)\n",
        "  rec_audio = np.array([])\n",
        "  for_reconstruction = mfb.fromLogMels(spectrogram)\n",
        "  for w in range(0,spectrogram.shape[0],hop):\n",
        "    spec = for_reconstruction[w:min(w+hop,for_reconstruction.shape[0]),:]\n",
        "  rec = rW.reconstructWavFromSpectrogram(spec,spec.shape[0]*spec.shape[1],fftsize=int(audiosr*winLength),overlap=int(winLength/frameshift))\n",
        "  rec_audio = np.append(rec_audio,rec)\n",
        "  scaled = np.int16(rec_audio/np.max(np.abs(rec_audio)) * 32767)\n",
        "  return scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2h23m2O-QHk"
      },
      "outputs": [],
      "source": [
        "audiosr = 16000\n",
        "winLength = 0.05\n",
        "frameshift = 0.01\n",
        "np.save(os.path.join(result_path,f'{pt}_predicted_spec_ensemble.npy'), ensemble)\n",
        "#Synthesize waveform from spectrogram using Griffin-Lim\n",
        "reconstructedWav = createAudio(ensemble,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "wavfile.write(os.path.join(result_path,f'{pt}_predicted_ensemble.wav'),int(audiosr),reconstructedWav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rghLZqek-QHk"
      },
      "outputs": [],
      "source": [
        "#For comparison synthesize the original spectrogram with Griffin-Lim\n",
        "origWav = createAudio(_spectrogram,audiosr=audiosr,winLength=winLength,frameshift=frameshift)\n",
        "wavfile.write(os.path.join(result_path,f'{pt}_orig_synthesized_ensemble.wav'),int(audiosr),origWav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b0Vf-hU-QHl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "print(\"Original synthesized\\n\")\n",
        "display(Audio(os.path.join(result_path,f'{pt}_orig_synthesized_ensemble.wav'), autoplay=False))\n",
        "print(\"\\nPredicted\\n\")\n",
        "display(Audio(os.path.join(result_path,f'{pt}_predicted_ensemble.wav'), autoplay=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}